from dragonmapper import hanzi, transcriptions
import jieba
from jisho_api.word import Word
from jisho_api.sentence import Sentence
import pandas as pd
import re
import requests 
import spacy
from spacy_streamlit import visualize_ner, visualize_tokens
#from spacy.language import Language
from spacy.tokens import Doc
import spacy_ke
from spacy_wordnet.wordnet_annotator import WordnetAnnotator 
import streamlit as st
import nltk

# Global variables
MODELS = {"ä¸­æ–‡": "zh_core_web_sm", 
          "English": "en_core_web_sm", 
          "æ—¥æœ¬èª": "ja_ginza"}
models_to_display = list(MODELS.keys())
ZH_TEXT = """ï¼ˆä¸­å¤®ç¤¾ï¼‰è¿æ¥è™å¹´åˆ°ä¾†ï¼Œå°åŒ—101ä»Šå¤©è¡¨ç¤ºï¼Œå³æ—¥èµ·æ¨å‡ºã€Œè™å¹´æ–°æ˜¥ç‡ˆå…‰ç§€ã€ï¼Œå°‡æŒçºŒè‡³2æœˆ5æ—¥ï¼Œæ¯æ™š6æ™‚è‡³10æ™‚ï¼Œé™¤æ•´é»æœƒæœ‰å ±æ™‚ç‡ˆå…‰è®ŠåŒ–å¤–ï¼Œæ¯15åˆ†é˜é‚„æœƒæœ‰3åˆ†é˜çš„ç‡ˆå…‰ç§€ã€‚å°åŒ—101ä¸‹åˆé€éæ–°èç¨¿è¡¨ç¤ºï¼Œä»Šå¹´ç‰¹åˆ¥è¨­è¨ˆã€Œè™å¹´æ–°æ˜¥ç‡ˆå…‰ç§€ã€ï¼Œå¾ä»Šæ™šé–‹å§‹é–ƒè€€å°åŒ—å¤©éš›ç·šï¼Œä¸€ç›´å»¶çºŒè‡³2æœˆ5æ—¥ï¼Œå…±7å¤©ã€‚"""
ZH_REGEX = "\d{2,4}[\u4E00-\u9FFF]+"
EN_TEXT = """(CNN) Residents of Taiwan's Rainbow Village are not your average fellow homo sapiens, but whimsical, brightly-colored animals.
Covered in vibrant colors and funky illustrations from the walls to the floor, the 1,000 square meter art park in Taichung, central Taiwan, has been an Instagrammers' favorite thanks to its kaleidoscopic visuals, attracting around two million visitors per year before the Covid-19 pandemic.
People don't visit just for its aesthetics, they also love its backstory: The village was once on the verge of demolition, but one veteran's simple action of painting saved it and gave it an even more glamourous second life."""
EN_REGEX = "(ed|ing)$"
JA_TEXT = """ï¼ˆæœæ—¥æ–°èï¼‰å°æ¹¾æ°—åˆ†ã®ãƒ‘ãƒ¯ãƒ¼ã‚¹ãƒãƒƒãƒˆ ï¼ªï¼²å¤§ä¹…ä¿é§…å—å£ã®ã™ããã°ã«ã‚ã‚‹ã€Œæ±äº¬åª½ç¥–å»Ÿï¼ˆã¾ãã³ã‚‡ã†ï¼‰ã€ã¯ã€å°æ¹¾ã§åºƒãä¿¡ä»°ã•ã‚Œã¦ã„ã‚‹é“æ•™ã®ç¥æ§˜ã‚’ç¥­ã‚‹ã€‚å±…é…’å±‹ã‚„ã‚³ãƒ³ãƒ“ãƒ‹ãŒä¸¦ã¶é€šã‚Šã§ã€é‡‘è‰²ã®ç«œãªã©è±ªè¯ãªè£…é£¾ãŒæ–½ã•ã‚ŒãŸï¼”éšå»ºã¦ã®èµ¤ã„å»ºç‰©ã¯ã¨ã¦ã‚‚ç›®ç«‹ã¤ã€‚"""
JA_REGEX = "[ãŸã„]$"
DESCRIPTION = "AIæ¨¡å‹è¼”åŠ©èªè¨€å­¸ç¿’"
TOK_SEP = " | "

# External API callers
def moedict_caller(word):
    st.write(f"### {word}")
    req = requests.get(f"https://www.moedict.tw/a/{word}.json")
    if req:
        with st.expander("é»æ“Š + æª¢è¦–çµæœ"):
            st.json(req.json())
    else:
        st.write("æŸ¥ç„¡çµæœ")

def parse_jisho_senses(word):
    res = Word.request(word)
    response = res.dict()
    if response["meta"]["status"] == 200:
        data = response["data"]
        commons = [d for d in data if d["is_common"]]
        if commons:
            common = commons[0] # Only get the first entry that is common
            senses = common["senses"]
            if len(senses) > 3:
                senses = senses[:3]
            with st.container():
                for idx, sense in enumerate(senses):
                    eng_def = "; ".join(sense["english_definitions"])
                    pos = "/".join(sense["parts_of_speech"])
                    st.write(f"Sense {idx+1}: {eng_def} ({pos})")
        else:
            st.info("Found no common words on Jisho!")
    else:
        st.error("Can't get response from Jisho!")


def parse_jisho_sentences(word):
    res = Sentence.request(word)
    try:
        response = res.dict()
        data = response["data"]
        if len(data) > 3:
            sents = data[:3]
        else:
            sents = data
        with st.container():
            for idx, sent in enumerate(sents):
                eng = sent["en_translation"]
                jap = sent["japanese"]
                st.write(f"Sentence {idx+1}: {jap}")
                st.write(f"({eng})")
    except:
        st.info("Found no results on Jisho!")
            
# Custom tokenizer class
class JiebaTokenizer:
    def __init__(self, vocab):
        self.vocab = vocab

    def __call__(self, text):
        words = jieba.cut(text) # returns a generator
        tokens = list(words) # convert the genetator to a list
        spaces = [False] * len(tokens)
        doc = Doc(self.vocab, words=tokens, spaces=spaces)
        return doc
    
# Utility functions
def create_jap_df(tokens):
    seen_texts = []
    filtered_tokens = []
    for tok in tokens:
        if tok.text not in seen_texts:
            filtered_tokens.append(tok)
            
    df = pd.DataFrame(
      {
          "å–®è©": [tok.text for tok in filtered_tokens],
          "ç™¼éŸ³": ["/".join(tok.morph.get("Reading")) for tok in filtered_tokens],
          "è©å½¢è®ŠåŒ–": ["/".join(tok.morph.get("Inflection")) for tok in filtered_tokens],
          "åŸå½¢": [tok.lemma_ for tok in filtered_tokens],
          #"æ­£è¦å½¢": [tok.norm_ for tok in verbs],
      }
    )
    st.dataframe(df)
    csv = df.to_csv().encode('utf-8')
    st.download_button(
      label="ä¸‹è¼‰è¡¨æ ¼",
      data=csv,
      file_name='jap_forms.csv',
      )

def create_eng_df(tokens):
    seen_texts = []
    filtered_tokens = []
    for tok in tokens:
        if tok.lemma_ not in seen_texts:
            filtered_tokens.append(tok)
            
    df = pd.DataFrame(
      {
          "å–®è©": [tok.text.lower() for tok in filtered_tokens],
          "è©é¡": [tok.pos_ for tok in filtered_tokens],
          "åŸå½¢": [tok.lemma_ for tok in filtered_tokens],
      }
    )
    st.dataframe(df)
    csv = df.to_csv().encode('utf-8')
    st.download_button(
      label="ä¸‹è¼‰è¡¨æ ¼",
      data=csv,
      file_name='eng_forms.csv',
      )
          
def filter_tokens(doc):
    clean_tokens = [tok for tok in doc if tok.pos_ not in ["PUNCT", "SYM"]]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_email]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_url]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_num]
    clean_tokens = [tok for tok in clean_tokens if not tok.is_punct]
    clean_tokens = [tok for tok in clean_tokens if not tok.is_space]
    return clean_tokens

def get_def_and_ex_from_wordnet(synsets):
    pass

def create_kw_section(doc):
    st.markdown("## é—œéµè©") 
    kw_num = st.slider("è«‹é¸æ“‡é—œéµè©æ•¸é‡", 1, 10, 3)
    kws2scores = {keyword: score for keyword, score in doc._.extract_keywords(n=kw_num)}
    kws2scores = sorted(kws2scores.items(), key=lambda x: x[1], reverse=True)
    count = 1
    for keyword, score in kws2scores: 
        rounded_score = round(score, 3)
        st.write(f"{count} >>> {keyword} ({rounded_score})")
        count += 1 
            
# Page setting
st.set_page_config(
    page_icon="ğŸ¤ ",
    layout="wide",
)

# Choose a language model
st.markdown(f"# {DESCRIPTION}") 
st.markdown("## èªè¨€æ¨¡å‹") 
selected_model = st.radio("è«‹é¸æ“‡èªè¨€", models_to_display)
nlp = spacy.load(MODELS[selected_model])
nlp.add_pipe("yake") # keyword extraction
          
# Merge entity spans to tokens
# nlp.add_pipe("merge_entities") 
st.markdown("---")

# Download NLTK data 
nltk.download('wordnet')
nltk.download('omw') # standing for Open Multilingual WordNet

# Default text and regex
st.markdown("## å¾…åˆ†ææ–‡æœ¬") 
if selected_model == models_to_display[0]: # Chinese
    # Select a tokenizer if the Chinese model is chosen
    selected_tokenizer = st.radio("è«‹é¸æ“‡æ–·è©æ¨¡å‹", ["jieba-TW", "spaCy"])
    if selected_tokenizer == "jieba-TW":
        nlp.tokenizer = JiebaTokenizer(nlp.vocab)
    default_text = ZH_TEXT
    default_regex = ZH_REGEX
elif selected_model == models_to_display[1]: # English
    default_text = EN_TEXT 
    default_regex = EN_REGEX 
elif selected_model == models_to_display[2]: # Japanese
    default_text = JA_TEXT
    default_regex = JA_REGEX 

st.info("ä¿®æ”¹æ–‡æœ¬å¾Œï¼ŒæŒ‰ä¸‹Ctrl + Enterä»¥æ›´æ–°")
text = st.text_area("",  default_text, height=300)
doc = nlp(text)
st.markdown("---")

# Two columns
left, right = st.columns(2)

with left:
    # Model output
    ner_labels = nlp.get_pipe("ner").labels
    visualize_ner(doc, labels=ner_labels, show_table=False, title="å‘½åå¯¦é«”")
    visualize_tokens(doc, attrs=["text", "pos_", "tag_", "dep_", "head"], title="æ–·è©ç‰¹å¾µ")
    st.markdown("---")

with right:
    punct_and_sym = ["PUNCT", "SYM"]
    if selected_model == models_to_display[0]: # Chinese 
        create_kw_section(doc)

        st.markdown("## åˆ†æå¾Œæ–‡æœ¬") 
        for idx, sent in enumerate(doc.sents):
            tokens_text = [tok.text for tok in sent if tok.pos_ not in punct_and_sym]
            pinyins = [hanzi.to_pinyin(word) for word in tokens_text]
            display = []
            for text, pinyin in zip(tokens_text, pinyins):
                res = f"{text} [{pinyin}]"
                display.append(res)
            if display:
              display_text = TOK_SEP.join(display)
              st.write(f"{idx+1} >>> {display_text}")
            else:
              st.write(f"{idx+1} >>> EMPTY LINE")
                
        st.markdown("## å–®è©è§£é‡‹")
        clean_tokens = filter_tokens(doc)
        alphanum_pattern = re.compile(r"[a-zA-Z0-9]")
        clean_tokens_text = [tok.text for tok in clean_tokens if not alphanum_pattern.search(tok.text)]
        vocab = list(set(clean_tokens_text))
        if vocab:
            selected_words = st.multiselect("è«‹é¸æ“‡è¦æŸ¥è©¢çš„å–®è©: ", vocab, vocab[0:3])
            for w in selected_words:
                moedict_caller(w)                        
                    
    elif selected_model == models_to_display[2]: # Japanese  
        create_kw_section(doc)

        st.markdown("## åˆ†æå¾Œæ–‡æœ¬") 
        for idx, sent in enumerate(doc.sents):
            clean_tokens = [tok for tok in sent if tok.pos_ not in ["PUNCT", "SYM"]]
            tokens_text = [tok.text for tok in clean_tokens]
            readings = ["/".join(tok.morph.get("Reading")) for tok in clean_tokens]
            display = [f"{text} [{reading}]" for text, reading in zip(tokens_text, readings)]
            if display:
              display_text = TOK_SEP.join(display)
              st.write(f"{idx+1} >>> {display_text}")
            else:
              st.write(f"{idx+1} >>> EMPTY LINE")  
                
        st.markdown("## å–®è©è§£é‡‹èˆ‡ä¾‹å¥")
        clean_tokens = filter_tokens(doc)
        alphanum_pattern = re.compile(r"[a-zA-Z0-9]")
        clean_lemmas = [tok.lemma_ for tok in clean_tokens if not alphanum_pattern.search(tok.lemma_)]
        vocab = list(set(clean_lemmas))
        if vocab:
            selected_words = st.multiselect("è«‹é¸æ“‡è¦æŸ¥è©¢çš„å–®è©: ", vocab, vocab[0:3])
            for w in selected_words:
                st.write(f"### {w}")
                with st.expander("é»æ“Š + æª¢è¦–çµæœ"):
                    parse_jisho_senses(w)
                    parse_jisho_sentences(w)

        st.markdown("## è©å½¢è®ŠåŒ–")
        # Collect inflected forms
        inflected_forms = [tok for tok in doc if tok.tag_.startswith("å‹•è©") or tok.tag_.startswith("å½¢")]
        if inflected_forms:
            create_jap_df(inflected_forms)

    elif selected_model == models_to_display[1]: # English                 
        create_kw_section(doc)
        
        nlp.add_pipe("spacy_wordnet", after='tagger', config={'lang': nlp.lang})
        doc = nlp(text)
        st.markdown("## åˆ†æå¾Œæ–‡æœ¬") 
        for idx, sent in enumerate(doc.sents):
            enriched_sentence = []
            for tok in sent:
                if tok.pos_ != "VERB":
                    enriched_sentence.append(tok.text)
                else:
                    synsets = tok._.wordnet.synsets()
                    if synsets:
                        v_synsets = [s for s in synsets if s.pos()=='v']
                        if not v_synsets:
                            enriched_sentence.append(tok.text)
                        else:
                            lemmas_for_synset = [lemma for s in v_synsets for lemma in s.lemma_names()]
                            lemmas_for_synset = list(set(lemmas_for_synset))

                            try:
                                lemmas_for_synset.remove(tok.text)
                            except:
                                pass

                            if len(lemmas_for_synset) > 5:
                                lemmas_for_synset = lemmas_for_synset[:5]

                            lemmas_for_synset = [s.replace("_", " ") for s in lemmas_for_synset]
                            lemmas_for_synset = " | ".join(lemmas_for_synset)
                            enriched_tok = f"{tok.text} (cf. {lemmas_for_synset})"
                            enriched_sentence.append(enriched_tok)  
                    
                
            display_text = " ".join(enriched_sentence)
            st.write(f"{idx+1} >>> {display_text}")     
            
        st.markdown("## å–®è©è§£é‡‹èˆ‡ä¾‹å¥")
        clean_tokens = filter_tokens(doc)
        num_pattern = re.compile(r"[0-9]")
        clean_lemmas = [tok.lemma_ for tok in clean_tokens if not num_pattern.search(tok.lemma_)]
        vocab = list(set(clean_lemmas))
        if vocab:
            selected_words = st.multiselect("è«‹é¸æ“‡è¦æŸ¥è©¢çš„å–®è©: ", vocab, vocab[0:3])
            for w in selected_words:
                st.write(f"### {w}")
                with st.expander("é»æ“Š + æª¢è¦–çµæœ"):
                    pass
                    
        st.markdown("## è©å½¢è®ŠåŒ–")
        # Collect inflected forms
        inflected_forms = [tok for tok in doc if tok.text.lower() != tok.lemma_.lower()]
        if inflected_forms:
            create_eng_df(inflected_forms)
