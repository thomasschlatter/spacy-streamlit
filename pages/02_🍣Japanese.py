from jisho_api.word import Word
from jisho_api.sentence import Sentence
import pandas as pd
import re
import requests 
import spacy
from spacy_streamlit import visualize_ner, visualize_tokens
#from spacy.language import Language
from spacy.tokens import Doc
import spacy_ke
import streamlit as st

# Global variables
DEFAULT_TEXT = """ãã‚Œã¾ã§ã€ã¼ãã¯ãšã£ã¨ã²ã¨ã‚Šã¼ã£ã¡ã ã£ãŸã€‚ã ã‚Œã¨ã‚‚ã†ã¡ã¨ã‘ã‚‰ã‚Œãªã„ã¾ã¾ã€ï¼–å¹´ã¾ãˆã€ã¡ã‚‡ã£ã¨ãŠã‹ã—ããªã£ã¦ã€ã‚µãƒãƒ©ã•ã°ãã«ä¸‹ã‚ŠãŸã€‚ã¼ãã®ã‚¨ãƒ³ã‚¸ãƒ³ã®ãªã‹ã§ã€ãªã«ã‹ãŒã“ã‚ã‚Œã¦ã„ãŸã€‚ã¼ãã«ã¯ã€ã¿ã¦ãã‚Œã‚‹ã²ã¨ã‚‚ã€ãŠãã‚ƒãã•ã‚“ã‚‚ã„ãªã‹ã£ãŸã‹ã‚‰ã€ãªãŠã™ã®ã¯ã‚€ãšã‹ã—ã„ã‘ã©ã€ãœã‚“ã¶ã²ã¨ã‚Šã§ãªã‚“ã¨ã‹ã‚„ã£ã¦ã¿ã‚‹ã“ã¨ã«ã—ãŸã€‚ãã‚Œã§ã¼ãã®ã„ã®ã¡ãŒãã¾ã£ã¦ã—ã¾ã†ã€‚ã®ã¿æ°´ã¯ã€ãŸã£ãŸï¼—æ—¥ã¶ã‚“ã—ã‹ãªã‹ã£ãŸã€‚
ã€€ï¼‘æ—¥ã‚ã®å¤œã€ã¼ãã¯ã™ãªã®ä¸Šã§ã­ã‚€ã£ãŸã€‚ã²ã¨ã®ã™ã‚€ã¨ã“ã‚ã¯ã€ã¯ã‚‹ã‹ã‹ãªãŸã ã£ãŸã€‚æµ·ã®ã©ã¾ã‚“ãªã‹ã€ã„ã‹ã ã§ã•ã¾ã‚ˆã£ã¦ã„ã‚‹ã²ã¨ã‚ˆã‚Šã‚‚ã€ã‚‚ã£ã¨ã²ã¨ã‚Šã¼ã£ã¡ã€‚ã ã‹ã‚‰ã€ã¼ããŒã³ã£ãã‚Šã—ãŸã®ã‚‚ã€ã¿ã‚“ãªã‚ã‹ã£ã¦ãã‚Œã‚‹ã¨ãŠã‚‚ã†ã€‚ã˜ã¤ã¯ã€ã‚ã•æ—¥ãŒã®ã¼ã‚‹ã“ã‚ã€ã¼ãã¯ã€ãµã—ããªã‹ã‚ã„ã„ã“ãˆã§ãŠã“ã•ã‚ŒãŸã‚“ã ã€‚
ã€Œã”ã‚ã‚“ãã ã•ã„â€¦â€¦ãƒ’ãƒ„ã‚¸ã®çµµã‚’ã‹ã„ã¦ï¼ã€
ã€Œãˆã£ï¼Ÿã€
ã€Œã¼ãã«ãƒ’ãƒ„ã‚¸ã®çµµã‚’ã‹ã„ã¦â€¦â€¦ã€
ã€æ˜Ÿã®ç‹å­ã•ã¾ã€"""
DESCRIPTION = "AIæ¨¡å‹è¼”åŠ©èªè¨€å­¸ç¿’ï¼šæ—¥èª"
TOK_SEP = " | "
MODEL_NAME = "ja_ginza"

# External API callers
def parse_jisho_senses(word):
    res = Word.request(word)
    response = res.dict()
    if response["meta"]["status"] == 200:
        data = response["data"]
        commons = [d for d in data if d["is_common"]]
        if commons:
            common = commons[0] # Only get the first entry that is common
            senses = common["senses"]
            if len(senses) > 3:
                senses = senses[:3]
            with st.container():
                for idx, sense in enumerate(senses):
                    eng_def = "; ".join(sense["english_definitions"])
                    pos = "/".join(sense["parts_of_speech"])
                    st.write(f"Sense {idx+1}: {eng_def} ({pos})")
        else:
            st.info("Found no common words on Jisho!")
    else:
        st.error("Can't get response from Jisho!")


def parse_jisho_sentences(word):
    res = Sentence.request(word)
    try:
        response = res.dict()
        data = response["data"]
        if len(data) > 3:
            sents = data[:3]
        else:
            sents = data
        with st.container():
            for idx, sent in enumerate(sents):
                eng = sent["en_translation"]
                jap = sent["japanese"]
                st.write(f"Sentence {idx+1}: {jap}")
                st.write(f"({eng})")
    except:
        st.info("Found no results on Jisho!")
    
# Utility functions
def create_jap_df(tokens):
    seen_texts = []
    filtered_tokens = []
    for tok in tokens:
        if tok.text not in seen_texts:
            filtered_tokens.append(tok)
            
    df = pd.DataFrame(
      {
          "å–®è©": [tok.text for tok in filtered_tokens],
          "ç™¼éŸ³": ["/".join(tok.morph.get("Reading")) for tok in filtered_tokens],
          "è©å½¢è®ŠåŒ–": ["/".join(tok.morph.get("Inflection")) for tok in filtered_tokens],
          "åŸå½¢": [tok.lemma_ for tok in filtered_tokens],
          #"æ­£è¦å½¢": [tok.norm_ for tok in verbs],
      }
    )
    st.dataframe(df)
    csv = df.to_csv().encode('utf-8')
    st.download_button(
      label="ä¸‹è¼‰è¡¨æ ¼",
      data=csv,
      file_name='jap_forms.csv',
      )
          
def filter_tokens(doc):
    clean_tokens = [tok for tok in doc if tok.pos_ not in ["PUNCT", "SYM"]]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_email]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_url]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_num]
    clean_tokens = [tok for tok in clean_tokens if not tok.is_punct]
    clean_tokens = [tok for tok in clean_tokens if not tok.is_space]
    return clean_tokens

def create_kw_section(doc):
    st.markdown("## é—œéµè©åˆ†æ") 
    kw_num = st.slider("è«‹é¸æ“‡é—œéµè©æ•¸é‡", 1, 10, 3)
    kws2scores = {keyword: score for keyword, score in doc._.extract_keywords(n=kw_num)}
    kws2scores = sorted(kws2scores.items(), key=lambda x: x[1], reverse=True)
    count = 1
    for keyword, score in kws2scores: 
        rounded_score = round(score, 3)
        st.write(f"{count} >>> {keyword} ({rounded_score})")
        count += 1 
            
# Page setting
st.set_page_config(
    page_icon="ğŸ¤ ",
    layout="wide",
    initial_sidebar_state="auto",
)
st.markdown(f"# {DESCRIPTION}") 

# Load the model
nlp = spacy.load(MODEL_NAME)

# Add pipelines to spaCy
nlp.add_pipe("yake") # keyword extraction
# nlp.add_pipe("merge_entities") # Merge entity spans to tokens

# Page starts from here
st.markdown("## å¾…åˆ†ææ–‡æœ¬")     
st.info("è«‹åœ¨ä¸‹é¢çš„æ–‡å­—æ¡†è¼¸å…¥æ–‡æœ¬ä¸¦æŒ‰ä¸‹Ctrl + Enterä»¥æ›´æ–°åˆ†æçµæœ")
text = st.text_area("",  DEFAULT_TEXT, height=200)
doc = nlp(text)
st.markdown("---")

st.info("è«‹å‹¾é¸ä»¥ä¸‹è‡³å°‘ä¸€é …åŠŸèƒ½")
keywords_extraction = st.checkbox("é—œéµè©åˆ†æ", False)
analyzed_text = st.checkbox("å¢å¼·æ–‡æœ¬", True)
defs_examples = st.checkbox("å–®è©è§£æ", True)
morphology = st.checkbox("è©å½¢è®ŠåŒ–", False)
ner_viz = st.checkbox("å‘½åå¯¦é«”", True)
tok_table = st.checkbox("æ–·è©ç‰¹å¾µ", False)

if keywords_extraction:
    create_kw_section(doc)

if analyzed_text:
    st.markdown("## åˆ†æå¾Œæ–‡æœ¬") 
    for idx, sent in enumerate(doc.sents):
        clean_tokens = [tok for tok in sent if tok.pos_ not in ["PUNCT", "SYM"]]
        tokens_text = [tok.text for tok in clean_tokens]
        readings = ["/".join(tok.morph.get("Reading")) for tok in clean_tokens]
        display = [f"{text} [{reading}]" for text, reading in zip(tokens_text, readings)]
        if display:
          display_text = TOK_SEP.join(display)
          st.write(f"{idx+1} >>> {display_text}")
        else:
          st.write(f"{idx+1} >>> EMPTY LINE")  

if defs_examples:
    st.markdown("## å–®è©è§£é‡‹èˆ‡ä¾‹å¥")
    clean_tokens = filter_tokens(doc)
    alphanum_pattern = re.compile(r"[a-zA-Z0-9]")
    clean_lemmas = [tok.lemma_ for tok in clean_tokens if not alphanum_pattern.search(tok.lemma_)]
    vocab = list(set(clean_lemmas))
    if vocab:
        selected_words = st.multiselect("è«‹é¸æ“‡è¦æŸ¥è©¢çš„å–®è©: ", vocab, vocab[0:3])
        for w in selected_words:
            st.write(f"### {w}")
            with st.expander("é»æ“Š + æª¢è¦–çµæœ"):
                parse_jisho_senses(w)
                parse_jisho_sentences(w)

if morphology:
    st.markdown("## è©å½¢è®ŠåŒ–")
    # Collect inflected forms
    inflected_forms = [tok for tok in doc if tok.tag_.startswith("å‹•è©") or tok.tag_.startswith("å½¢")]
    if inflected_forms:
        create_jap_df(inflected_forms)

if ner_viz:
    ner_labels = nlp.get_pipe("ner").labels
    visualize_ner(doc, labels=ner_labels, show_table=False, title="å‘½åå¯¦é«”")

if tok_table:
    visualize_tokens(doc, attrs=["text", "pos_", "tag_", "dep_", "head"], title="æ–·è©ç‰¹å¾µ")
       
