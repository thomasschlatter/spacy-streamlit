from collections import Counter
from dragonmapper import hanzi, transcriptions
import jieba
import pandas as pd
import plotly.express as px
import re
import requests 
import spacy
from spacy_streamlit import visualize_ner, visualize_tokens
#from spacy.language import Language
from spacy.tokens import Doc
import streamlit as st

# Global variables
ZH_TEXT = """ï¼ˆä¸­å¤®ç¤¾ï¼‰è¿æ¥è™å¹´åˆ°ä¾†ï¼Œå°åŒ—101ä»Šå¤©è¡¨ç¤ºï¼Œå³æ—¥èµ·æ¨å‡ºã€Œè™å¹´æ–°æ˜¥ç‡ˆå…‰ç§€ã€ï¼Œå°‡æŒçºŒè‡³2æœˆ5æ—¥ï¼Œæ¯æ™š6æ™‚è‡³10æ™‚ï¼Œé™¤æ•´é»æœƒæœ‰å ±æ™‚ç‡ˆå…‰è®ŠåŒ–å¤–ï¼Œæ¯15åˆ†é˜é‚„æœƒæœ‰3åˆ†é˜çš„ç‡ˆå…‰ç§€ã€‚å°åŒ—101ä¸‹åˆé€éæ–°èç¨¿è¡¨ç¤ºï¼Œä»Šå¹´ç‰¹åˆ¥è¨­è¨ˆã€Œè™å¹´æ–°æ˜¥ç‡ˆå…‰ç§€ã€ï¼Œå¾ä»Šæ™šé–‹å§‹é–ƒè€€å°åŒ—å¤©éš›ç·šï¼Œä¸€ç›´å»¶çºŒè‡³2æœˆ5æ—¥ï¼Œå…±7å¤©ã€‚"""
DESCRIPTION = "AIæ¨¡å‹è¼”åŠ©èªè¨€å­¸ç¿’"
TOK_SEP = " | "
PUNCT_SYM = ["PUNCT", "SYM"]

# External API callers
def moedict_caller(word):
    st.write(f"### {word}")
    req = requests.get(f"https://www.moedict.tw/uni/{word}.json")
    try:
        definitions = req.json().get('heteronyms')[0].get('definitions')
        df = pd.DataFrame(definitions)
        df.fillna("---", inplace=True)
        if 'example' not in df.columns:
            df['example'] = '---'
        if 'synonyms' not in df.columns:
            df['synonyms'] = '---' 
        if 'antonyms' not in df.columns:
            df['antonyms'] = '---' 
        cols = ['def', 'example', 'synonyms', 'antonyms']
        df = df[cols]
        df.rename(columns={
            'def': 'è§£é‡‹',
            'example': 'ä¾‹å¥',
            'synonyms': 'åŒç¾©è©',
            'antonyms': 'åç¾©è©',
        }, inplace=True)
        with st.expander("é»æ“Š + æŸ¥çœ‹çµæœ"):
            st.table(df)
    except:
        st.write("æŸ¥ç„¡çµæœ")
            
# Custom tokenizer class
class JiebaTokenizer:
    def __init__(self, vocab):
        self.vocab = vocab

    def __call__(self, text):
        words = jieba.cut(text) # returns a generator
        tokens = list(words) # convert the genetator to a list
        spaces = [False] * len(tokens)
        doc = Doc(self.vocab, words=tokens, spaces=spaces)
        return doc
    
# Utility functions
def filter_tokens(doc):
    clean_tokens = [tok for tok in doc if tok.pos_ not in PUNCT_SYM]
    clean_tokens = (
        [tok for tok in clean_tokens if 
         not tok.like_email and 
         not tok.like_num and 
         not tok.like_url and 
         not tok.is_space]
    )
    return clean_tokens

def get_vocab(doc):
    clean_tokens = filter_tokens(doc)
    alphanum_pattern = re.compile(r"[a-zA-Z0-9]")
    clean_tokens_text = [tok.text for tok in clean_tokens if not alphanum_pattern.search(tok.text)]
    vocab = list(set(clean_tokens_text))
    return vocab

def get_counter(doc):
    clean_tokens = filter_tokens(doc)
    tokens = [token.text for token in clean_tokens]
    counter = Counter(tokens)
    return counter

def get_freq_fig(doc):
    counter = get_counter(doc)
    counter_df = (
        pd.DataFrame.from_dict(counter, orient='index').
        reset_index().
        rename(columns={
            0: 'count', 
            'index': 'word'
            }).
        sort_values(by='count', ascending=False)
        )
    fig = px.bar(counter_df, x='word', y='count')
    return fig

def get_level_pie(tocfl_result):
    level = tocfl_result['è©æ¢åˆ†ç´š'].value_counts()
    fig = px.pie(tocfl_result, 
                values=level.values, 
                names=level.index, 
                title='è©å½™åˆ†ç´šåœ“é¤…åœ–')
    return fig

@st.cache
def load_tocfl_table(filename="./tocfl_wordlist.csv"):
    table = pd.read_csv(filename)
    cols = "è©å½™ æ¼¢èªæ‹¼éŸ³ æ³¨éŸ³ ä»»å‹™é ˜åŸŸ è©æ¢åˆ†ç´š".split()
    table = table[cols]
    return table
       
# Page setting
st.set_page_config(
    page_icon="ğŸ¤ ",
    layout="wide",
    initial_sidebar_state="auto",
)

# Choose a language and select functions
st.markdown(f"# {DESCRIPTION}") 

# Load the model
nlp = spacy.load('zh_core_web_sm')
          
# Merge entity spans to tokens
# nlp.add_pipe("merge_entities") 

# Select a tokenizer if the Chinese model is chosen
selected_tokenizer = st.radio("è«‹é¸æ“‡æ–·è©æ¨¡å‹", ["jieba-TW", "spaCy"])
if selected_tokenizer == "jieba-TW":
    nlp.tokenizer = JiebaTokenizer(nlp.vocab)
default_text = ZH_TEXT

st.markdown("## å¾…åˆ†ææ–‡æœ¬")     
st.info("è«‹åœ¨ä¸‹é¢çš„æ–‡å­—æ¡†è¼¸å…¥æ–‡æœ¬ä¸¦æŒ‰ä¸‹Ctrl + Enterä»¥æ›´æ–°åˆ†æçµæœ")
text = st.text_area("",  default_text, height=200)
doc = nlp(text)
st.markdown("---")

# keywords_extraction = st.sidebar.checkbox("é—œéµè©åˆ†æ", False) # YAKE doesn't work for Chinese texts
st.info("è«‹å‹¾é¸ä»¥ä¸‹è‡³å°‘ä¸€é …åŠŸèƒ½")
analyzed_text = st.checkbox("å¢å¼·æ–‡æœ¬", True)
defs_examples = st.checkbox("å–®è©è§£æ", True)
# morphology = st.sidebar.checkbox("è©å½¢è®ŠåŒ–", True)
freq_count = st.checkbox("è©é »çµ±è¨ˆ", True)
ner_viz = st.checkbox("å‘½åå¯¦é«”", True)
tok_table = st.checkbox("æ–·è©ç‰¹å¾µ", False)

if analyzed_text:
    st.markdown("## å¢å¼·æ–‡æœ¬") 
    pronunciation = st.radio("è«‹é¸æ“‡è¼”åŠ©ç™¼éŸ³é¡å‹", ["æ¼¢èªæ‹¼éŸ³", "æ³¨éŸ³ç¬¦è™Ÿ", "åœ‹éš›éŸ³æ¨™"])
    for idx, sent in enumerate(doc.sents):
        tokens_text = [tok.text for tok in sent if tok.pos_ not in PUNCT_SYM]
        pinyins = [hanzi.to_pinyin(word) for word in tokens_text]
        sounds = pinyins
        if pronunciation == "æ³¨éŸ³ç¬¦è™Ÿ":
            zhuyins = [transcriptions.pinyin_to_zhuyin(word) for word in pinyins]
            sounds = zhuyins
        elif pronunciation == "åœ‹éš›éŸ³æ¨™":
            ipas = [transcriptions.pinyin_to_ipa(word) for word in pinyins]
            sounds = ipas

        display = []
        for text, sound in zip(tokens_text, sounds):
            res = f"{text} [{sound}]"
            display.append(res)
        if display:
            display_text = TOK_SEP.join(display)
            st.write(f"{idx+1} >>> {display_text}")
        else:
            st.write(f"{idx+1} >>> EMPTY LINE")

if defs_examples:
    st.markdown("## å–®è©è§£æ")
    vocab = get_vocab(doc)
    if vocab:
        tocfl_table = load_tocfl_table()
        filt = tocfl_table['è©å½™'].isin(vocab)
        tocfl_res = tocfl_table[filt]
        st.markdown("### è¯èªè©å½™åˆ†ç´š")
        fig = get_level_pie(tocfl_res)
        st.plotly_chart(fig, use_container_width=True)

        with st.expander("é»æ“Š + æŸ¥çœ‹çµæœ"):
            st.table(tocfl_res)
        st.markdown("---")
        st.markdown("### å–®è©è§£é‡‹èˆ‡ä¾‹å¥")
        selected_words = st.multiselect("è«‹é¸æ“‡è¦æŸ¥è©¢çš„å–®è©: ", vocab, vocab[-1])
        for w in selected_words:
            moedict_caller(w)                        

if freq_count:  
    st.markdown("## è©é »çµ±è¨ˆ")  
    counter = get_counter(doc)
    topK = st.slider('è«‹é¸æ“‡å‰Kå€‹é«˜é »è©', 1, len(counter), 5)
    most_common = counter.most_common(topK)
    st.write(most_common)
    st.markdown("---")

    fig = get_freq_fig(doc)
    st.plotly_chart(fig, use_container_width=True)

if ner_viz:
    ner_labels = nlp.get_pipe("ner").labels
    visualize_ner(doc, labels=ner_labels, show_table=False, title="å‘½åå¯¦é«”")
    
if tok_table:
    visualize_tokens(doc, attrs=["text", "pos_", "tag_", "dep_", "head"], title="æ–·è©ç‰¹å¾µ")
